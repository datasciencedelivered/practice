{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb6989a-730f-47a4-8682-86bb3d2ff4fe",
   "metadata": {},
   "source": [
    "Process:\n",
    "1. read the vocab file and build the vacab\n",
    "2. split the data set\n",
    "3. create the dataset in reqluired format\n",
    "4. tokenize the and pad sequence\n",
    "5. crate model and train and save model\n",
    "6. use for prediction\n",
    "\n",
    "Batch Size: Choose based on memory capacity and training stability. Common values are 32 or 64.\n",
    "Sequence Length: Reflects the typical length of the text input, typically 100-300 words for reviews.\n",
    "Embedding Dimension: Affects the richness of word representations, with common values ranging from 50 to 300.\n",
    "\n",
    "hidden_dim in LSTM: Controls the size of the hidden states and the cell states. It determines the model's capacity to learn and retain information from sequences.\n",
    "Choosing hidden_dim: Balance between too small (which may underfit) and too large (which may overfit and require more computation).\n",
    "Impact: Larger hidden_dim increases the model's ability to capture complex patterns but also increases training time and memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b70622-c7f5-4198-88b4-31959de8bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Prepare the Data\n",
    "# Here, text contains the review and label contains the sentiment (1 for positive, 0 for negative).\n",
    "text,label\n",
    "\"I love this movie!\",1\n",
    "\"This is terrible.\",0\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53177350-6d0d-4774-a35c-46d3e1086202",
   "metadata": {},
   "source": [
    "Load and Preprocess Data\n",
    "Read the CSV File: Use pandas to read the file.\n",
    "Tokenize: Convert text into numerical tokens.\n",
    "Pad Sequences: Make all sequences the same length.\n",
    "Convert to Tensors: Prepare the data for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fec35-daaa-45b0-9a5c-81162394206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a4864-89d6-4f8a-80ca-53084939c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv('sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32f0b-c823-4710-b07e-ccf0b2d7ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54175a0f-0590-45a3-8b56-df00d2a024fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocab(sentences, max_vocab_size=25000):\n",
    "    words = Counter()\n",
    "    for sentence in sentences:\n",
    "        words.update(word_tokenize(sentence))\n",
    "    common_words = words.most_common(max_vocab_size)\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(common_words)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fb61c-0c13-4273-9836-a5a94258e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(sentence, vocab, max_length=100):\n",
    "    tokens = [vocab.get(word, vocab['<UNK>']) for word in word_tokenize(sentence)]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([vocab['<PAD>']] * (max_length - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e079af-c4d7-4c93-a91a-8c5d75be4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_length=100):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        tokens = tokenize_and_pad(text, self.vocab, self.max_length)\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba22e48-8e6c-498f-89ae-d7cfe018d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab(train_data['text'].tolist())\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SentimentDataset(train_data, vocab)\n",
    "test_dataset = SentimentDataset(test_data, vocab)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a851a-a9ca-46e4-8aac-0b31d7fef3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN Model\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths=[len(x)], batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "Explanation\n",
    "Embedding Layer: Converts tokens into dense vectors.\n",
    "RNN Layer: Processes the sequence of embeddings.\n",
    "Fully Connected Layer: Produces the final output.\n",
    "Training Loop: Updates the model parameters based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5e303-2dfe-4e1b-b14d-24350f666b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the RNN Model\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch\n",
    "        predictions = rnn_model(text).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-deep-learn)",
   "language": "python",
   "name": "venv-deep-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

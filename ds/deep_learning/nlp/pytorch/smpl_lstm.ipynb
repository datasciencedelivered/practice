{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb6989a-730f-47a4-8682-86bb3d2ff4fe",
   "metadata": {},
   "source": [
    "Process:\n",
    "1. read the vocab file and build the vacab\n",
    "2. split the data set\n",
    "3. create the dataset in required format\n",
    "4. tokenize the and pad sequence\n",
    "5. crate model and train and save model\n",
    "6. use for prediction\n",
    "\n",
    "Batch Size: Choose based on memory capacity and training stability. Common values are 32 or 64.\n",
    "Sequence Length: Reflects the typical length of the text input, typically 100-300 words for reviews.\n",
    "Embedding Dimension: Affects the richness of word representations, with common values ranging from 50 to 300.\n",
    "\n",
    "hidden_dim in LSTM: Controls the size of the hidden states and the cell states. It determines the model's capacity to learn and retain information from sequences.\n",
    "Choosing hidden_dim: Balance between too small (which may underfit) and too large (which may overfit and require more computation).\n",
    "Impact: Larger hidden_dim increases the model's ability to capture complex patterns but also increases training time and memory usage\n",
    "\n",
    "## Parameters to Consider:\n",
    "1. Batch Size (batch_size):\n",
    "    Represents the number of samples processed before the model updates its parameters.\n",
    "    Affects the training time and model performance.\n",
    "    Larger batch sizes lead to faster training but require more memory.\n",
    "    Smaller batch sizes can provide more stable gradient updates but may lead to noisier gradient estimates.\n",
    "\n",
    "2. Sequence Length (seq_length):\n",
    "    The number of words in each input sequence.\n",
    "    Affects how much context the model captures from the input text.\n",
    "    Should be chosen based on the typical length of input texts and memory constraints.\n",
    "\n",
    "Embedding Dimension (embedding_dim):\n",
    "\n",
    "Size of the word vectors representing the input words.\n",
    "Affects how much information is captured about each word.\n",
    "Higher dimensions can capture more information but may lead to overfitting and increased computation.\n",
    "\n",
    "The hidden_dim (hidden dimension) in an LSTM (Long Short-Term Memory) model refers to the number of features in the hidden state of the LSTM. It is a key parameter that determines the capacity and complexity of the model to capture patterns and dependencies in sequential data. Let's delve into what hidden_dim represents and how it affects the model's performance.\n",
    "\n",
    "Understanding hidden_dim in LSTM\n",
    "1. Hidden State Representation\n",
    "Hidden State (h_t): In an LSTM, the hidden state at each time step is a vector of length hidden_dim. It captures information about the sequence up to that point.\n",
    "Cell State (c_t): Along with the hidden state, the LSTM maintains a cell state, which is also a vector of length hidden_dim. The cell state helps in capturing long-term dependencies.\n",
    "2. Role in LSTM Architecture\n",
    "Output Size: The hidden state size determines the output size of each LSTM cell. If hidden_dim is 256, each LSTM cell will output a vector of 256 features for each time step.\n",
    "Complexity: Larger hidden_dim means the model has more parameters and can capture more complex patterns, but it also increases the risk of overfitting and requires more computational resources.\n",
    "Performance: A well-chosen hidden_dim can significantly improve the model's ability to learn and generalize from the data.\n",
    "Impact of hidden_dim on Model\n",
    "Memory Usage: Larger hidden_dim values lead to increased memory consumption.\n",
    "Training Time: Higher values can slow down training due to the increased number of computations per time step.\n",
    "Generalization: Overly large hidden_dim can lead to overfitting, while too small values may underfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee72cc-965d-4298-be96-798c63a9d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99fd3a-b393-4a7b-a134-2b28c8fe04c7",
   "metadata": {},
   "source": [
    "#Step 1: Prepare the Data\n",
    "# Here, text contains the review and label contains the sentiment (1 for positive, 0 for negative).\n",
    "text,label\n",
    "\"I love this movie!\",1\n",
    "\"This is terrible.\",0\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53177350-6d0d-4774-a35c-46d3e1086202",
   "metadata": {},
   "source": [
    "Load and Preprocess Data\n",
    "Read the CSV File: Use pandas to read the file.\n",
    "Tokenize: Convert text into numerical tokens.\n",
    "Pad Sequences: Make all sequences the same length.\n",
    "Convert to Tensors: Prepare the data for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647270c0-4856-479a-bb21-0b162c0a1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945fec35-daaa-45b0-9a5c-81162394206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764a4864-89d6-4f8a-80ca-53084939c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv('sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fe3e3-9d04-4623-9138-3594f94146d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df622037-8bc4-4991-9f45-16cee3427162",
   "metadata": {},
   "source": [
    "\n",
    "#print(word_tokenize(\"HI am a sentence\")) #['HI', 'am', 'a', 'sentence']\n",
    "words = Counter()\n",
    "print(words.update(word_tokenize(\"HI am a sentence\")))\n",
    "print(words.items())  ## dict_items([('HI', 1), ('am', 1), ('a', 1), ('sentence', 1)])\n",
    "print(words.values())  ## dict_values([1, 1, 1, 1])\n",
    "print(words.elements()) \n",
    "for element in words.elements():\n",
    "    print(element)\n",
    "print(words.most_common()) ## [('HI', 1), ('am', 1), ('a', 1), ('sentence', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0693dc1-ca14-47a4-bc9b-cb76840afe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocab(sentences, max_vocab_size=25000):\n",
    "    words = Counter()\n",
    "    for sentence in sentences:\n",
    "        words.update(word_tokenize(sentence))\n",
    "    common_words = words.most_common(max_vocab_size)\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(common_words)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fc311-9208-48ac-94e0-dfe19cdb316c",
   "metadata": {},
   "source": [
    "print(build_vocab([\"My sentence\", \"another sentence\"]))\n",
    "{'sentence': 2, 'My': 3, 'another': 4, '<PAD>': 0, '<UNK>': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "694fb61c-0c13-4273-9836-a5a94258e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(sentence, vocab, max_length=100):\n",
    "    tokens = [vocab.get(word, vocab['<UNK>']) for word in word_tokenize(sentence)]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([vocab['<PAD>']] * (max_length - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad2df2a-e00d-4a93-8e49-7966f1a4ba22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5          No\n",
       "2    Liked it\n",
       "4       Sorry\n",
       "3    Pathetic\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']\n",
    "train_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02e079af-c4d7-4c93-a91a-8c5d75be4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_length=100):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        tokens = tokenize_and_pad(text, self.vocab, self.max_length)\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452439f-6b33-4fcc-92e5-23513f9f90a3",
   "metadata": {},
   "source": [
    "print(train_data.iloc[:1,:])\n",
    "print(type(train_data))\n",
    "train_data.head()\n",
    "vacab = build_vocab([\"My sentence\", \"another sentence\"])\n",
    "sample_data = SentimentDataset(train_data.iloc[:1,:],vacab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cba22e48-8e6c-498f-89ae-d7cfe018d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No']\n",
      "['Liked', 'it']\n",
      "['Sorry']\n",
      "['Pathetic']\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab(train_data['text'].tolist())\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SentimentDataset(train_data, vocab)\n",
    "test_dataset = SentimentDataset(test_data, vocab)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "354a851a-a9ca-46e4-8aac-0b31d7fef3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths=[len(x)], batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01c5e303-2dfe-4e1b-b14d-24350f666b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.695932611823082\n",
      "Epoch 2, Loss: 0.6502176076173782\n",
      "Epoch 3, Loss: 0.6094500869512558\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch\n",
    "        predictions = lstm_model(text).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590a9a6-4372-4cd9-bbff-1f8833fb9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47996be9-300b-478b-b21e-704979a68e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': [\"I love this movie\", \"I hate this movie\", \"This movie is great\", \"This movie is terrible\"],\n",
    "    'label': [1, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40195da5-d3ce-41fd-96f5-2dfc3b6f0757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "raw model output : {} tensor([[0.0459]])\n",
      "raw model output after scraping dimension : {} tensor([0.0459])\n",
      "output after simoid : {} tensor([0.5115])\n",
      "Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "# text1 = [\"Not like\"]\n",
    "new_data = {\n",
    "    'text': [\"Not like\"],\n",
    "    'label': [0]  # Dummy label\n",
    "}\n",
    "predict_data = pd.DataFrame(new_data)\n",
    "#text1_df = pd.DataFrame([{\"text\":text1,\"label\": None}])\n",
    "predict_dataset = SentimentDataset(predict_data,vocab, max_length=10)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "chk_loader = DataLoader(predict_dataset, batch_size=1, shuffle = False)\n",
    "for batch in chk_loader:\n",
    "    print(type(batch))\n",
    "          \n",
    "lstm_model.eval()  # Set the model to evaluation mode\n",
    "for batch in chk_loader:\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output = lstm_model(batch[0])  # Forward pass through the model\n",
    "        print(\"raw model output : {}\",output)\n",
    "        output = output.squeeze(1)  # Remove any extra dimensions if necessary\n",
    "        print(\"raw model output after scraping dimension : {}\",output)\n",
    "        prediction = torch.sigmoid(output)  # Apply sigmoid to get probabilities\n",
    "        print(\"output after simoid : {}\",prediction)\n",
    "        predicted_label = (prediction > 0.5).float()  # Convert probabilities to binary labels\n",
    "\n",
    "print(\"Prediction:\", predicted_label.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-deep-learn)",
   "language": "python",
   "name": "venv-deep-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

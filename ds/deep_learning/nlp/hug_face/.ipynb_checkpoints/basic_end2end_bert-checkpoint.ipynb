{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7c3b5f-850f-4c3b-b1b3-6b6582d1a167",
   "metadata": {},
   "source": [
    "## This is a simple end to end example for Huggingface based transformer model called Bert\n",
    "## Also shows retraining, saving of a model and usage for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c9b23-a760-4f07-9e0f-795514e83eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Setup Environment\n",
    "#\tpip install torch torchvision torchaudio\n",
    "#\tpip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f018412-6e30-4e20-89cd-2544df73052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Import Libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea76ef-2994-42ec-9721-689a6542143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Load and Preprocess Dataset\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Use only a subset for training to keep it quick\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select([i for i in range(1000)])\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select([i for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5ed66-41b1-48f9-8c69-ccd232dc8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Tokenize the Data\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d59b6-9ef4-45e0-b7db-55f37c435311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Create Data Loaders\n",
    "# Create PyTorch data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6db6ae-c3b2-4cf4-a5ca-74754ae58cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Define the Model\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed18040-c761-41f1-9c5f-9bad2425e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Define Training Arguments and Trainer\n",
    "#The Trainer class simplifies the training process significantly.\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset            # Evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d282d-5a48-4cf4-a8a8-185d985d8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Train the Model\n",
    "\t# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c500d-142f-4202-873b-3ec68c530d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Evaluate the Model\n",
    "\t# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44795b53-b0ad-441f-bfed-5817f5e9aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Save the Model\n",
    "\t# Save the fine-tuned model\n",
    "model.save_pretrained('./sentiment_model')\n",
    "tokenizer.save_pretrained('./sentiment_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ab339-dbb3-4cf0-a2bd-2ee1dfd2cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Load the Model for Inference\n",
    "#\tFor making predictions on new data, you can load the model and tokenizer.\n",
    "\t\n",
    "# Load the model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./sentiment_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./sentiment_tokenizer')\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    return \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "# Example prediction\n",
    "print(predict_sentiment(\"This movie was amazing!\"))\n",
    "print(predict_sentiment(\"I did not like this movie at all.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4db27-c622-4e23-adda-daa5e36d3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. outputs.logits\n",
    "\tLogits: These are the raw scores produced by the model before applying any activation function like softmax. They represent the model's confidence in each class (in this case, sentiment classes)\t\n",
    "\t\n",
    "#13. outputs = model(**inputs)\t\n",
    "\tModel: The model is a pre-trained transformer model loaded from the Hugging Face library.\n",
    "\tInputs: The **inputs syntax unpacks the dictionary of inputs, which typically includes input_ids and attention_mask,\n",
    "    and passes them to the model for inference.\n",
    "\tOutputs: The model returns an object that includes various outputs, with logits being the raw, unnormalized scores \n",
    "    for each class.\n",
    "\t\n",
    "#14. Predicting the Sentiment:\n",
    "\tprediction = torch.argmax(logits, dim=-1).item()\n",
    "\t\n",
    "\ttorch.argmax: Finds the index of the highest value in logits, which corresponds to the predicted class. \n",
    "    dim=-1 specifies that the operation is performed across the last dimension, which is the class dimension.\n",
    "\titem(): Converts the PyTorch tensor to a standard Python integer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-deep-learn)",
   "language": "python",
   "name": "venv-deep-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

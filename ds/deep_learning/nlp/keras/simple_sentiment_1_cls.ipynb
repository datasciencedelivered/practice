{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af23dee-60f4-49ff-b55c-a0e279944725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 13:17:44.930179: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-13 13:17:44.990822: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-13 13:17:44.992293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 13:17:46.040645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9340bc4-2d79-4d7e-9719-6126453e0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "texts = [\n",
    "\t\t\"I love natural language processing.\",\n",
    "\t\t\"Natural language processing is fascinating.\",\n",
    "\t\t\"NLP is a subfield of artificial intelligence.\",\n",
    "\t\t\"I dislike bugs in my code.\",\n",
    "\t\t\"Sentiment analysis is not always easy.\",\n",
    "        \"Feel terrible to work on weekend\"\n",
    "\t]\n",
    "\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca328fef-6c28-4364-9da0-33602342a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts) # creates index of the texts word \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "## Now, we can use texts_to_sequences() to convert the texts into sequences:\n",
    "## The output will be a list of sequences, where each sequence represents a text converted into a sequence of integers based on the tokenizer's word index:\n",
    "## The first text \"I love machine learning\" is converted to the sequence [1, 2, 3, 4].\n",
    "## The second text \"Deep learning is amazing\" is converted to the sequence [5, 6, 7, 8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f548f4af-3e3d-48ed-b54c-3cee6a6d91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6, 3, 4, 5], [3, 4, 5, 1, 7], [8, 1, 9, 10, 11, 12, 13], [2, 14, 15, 16, 17, 18], [19, 20, 1, 21, 22, 23], [24, 25, 26, 27, 28, 29]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1bfa7c-8ac5-48b1-9456-d77d30da05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'i': 2, 'natural': 3, 'language': 4, 'processing': 5, 'love': 6, 'fascinating': 7, 'nlp': 8, 'a': 9, 'subfield': 10, 'of': 11, 'artificial': 12, 'intelligence': 13, 'dislike': 14, 'bugs': 15, 'in': 16, 'my': 17, 'code': 18, 'sentiment': 19, 'analysis': 20, 'not': 21, 'always': 22, 'easy': 23, 'feel': 24, 'terrible': 25, 'to': 26, 'work': 27, 'on': 28, 'weekend': 29}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbd9656-06e0-46ba-be8a-51f7a70515b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3863f294-7a90-4061-aadc-f39f1da4ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  2  6  3  4  5]\n",
      " [ 0  0  3  4  5  1  7]\n",
      " [ 8  1  9 10 11 12 13]\n",
      " [ 0  2 14 15 16 17 18]\n",
      " [ 0 19 20  1 21 22 23]\n",
      " [ 0 24 25 26 27 28 29]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9298c06b-c99d-42ea-8114-72c01a0ab553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(6, 7)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(labels))\n",
    "print(X.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af53a0f-e97b-418b-ae2c-48d1748d28c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 0.6946 - accuracy: 0.5000\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6850 - accuracy: 1.0000\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6775 - accuracy: 1.0000\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6696 - accuracy: 1.0000\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6607 - accuracy: 1.0000\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6504 - accuracy: 1.0000\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6388 - accuracy: 1.0000\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6256 - accuracy: 1.0000\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6109 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5944 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5763 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5563 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5346 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5110 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4850 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4570 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4272 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3959 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3635 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3305 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2975 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2649 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2333 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2030 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1743 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1228 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1004 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4eb67ee4c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Bag of Words model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=X.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, labels, epochs=40, batch_size=32) # Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9797ab-51e0-4aba-ba28-7cfd08cb0527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggupte/practice/ds/deep_learning/venv-deep-learn/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('models/sentiment_model.h5') # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6185cb-4e1e-418a-8c21-964f62c25970",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('models/sentiment_model.h5') # Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa0cd79-8e83-4e78-8245-56af925221af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded model for predictions on new data\n",
    "new_texts = [\"I enjoy working with natural language processing.\", \"This movie is terrible.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ad09bb-5e49-4330-8dfc-ec28eea2d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the new texts\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_data = pad_sequences(new_sequences, maxlen=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da551c51-34ba-486f-9176-5410fec3dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "[[0.9878718]\n",
      " [0.8670116]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model.predict(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b29ae8-1fb0-49da-a486-d4282cae73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predictions to labels (0 for negative, 1 for positive)\n",
    "predicted_labels = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0529de1d-a104-4893-b0dd-1ba9163e6f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I enjoy working with natural language processing. | Sentiment: Positive\n",
      "Text: This movie is terrible. | Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "for text, label in zip(new_texts, predicted_labels):\n",
    "\tsentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "\tprint(f\"Text: {text} | Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a514f-3e68-4cc4-8b3b-f4d592be0adf",
   "metadata": {},
   "source": [
    "text_to_sequences Function:\n",
    "\tInput:\n",
    "\ttexts: This is the input text data that you want to convert to sequences. It can be a list of strings, where each string represents a piece of text.\n",
    "\tOutput:\n",
    "\tsequences: The output of text_to_sequences is a list of lists. Each inner list corresponds to a sequence of integer indices representing the words in the corresponding input text. The indices are assigned based on the vocabulary learned by the Tokenizer.\n",
    "\n",
    "pad_sequences Function:\n",
    "\tInput:\n",
    "\tsequences: This is the list of lists obtained from the text_to_sequences function. Each inner list represents a sequence of integer indices.\n",
    "\tmaxlen (optional): This is an integer, the maximum length of all sequences. If provided, sequences longer than maxlen will be truncated, and sequences shorter than maxlen will be padded.\n",
    "\tpadding (optional): String, 'pre' or 'post'. If not provided, the default is 'pre'. If 'pre', padding is added at the beginning of the sequences; if 'post', padding is added at the end.\n",
    "\ttruncating (optional): String, 'pre' or 'post'. If not provided, the default is 'pre'. If 'pre', sequences longer than maxlen are truncated at the beginning; if 'post', sequences longer than maxlen are truncated at the end.\n",
    "\tvalue (optional): Float or String, the padding value. If not provided, the default is 0.0.\n",
    "\tOutput:\n",
    "\tpadded_sequences: The output of pad_sequences is a 2D numpy array where each row represents a padded or truncated sequence of integer indices. The array can be directly used as input to a neural network.\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-deep-learn)",
   "language": "python",
   "name": "venv-deep-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
